{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e960490f",
   "metadata": {},
   "source": [
    "# SageMaker JumpStart Foundation Models - Fine-tuning text generation GPT-J 6B model on domain specific dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7e72e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f2327e",
   "metadata": {},
   "source": [
    "---\n",
    "Welcome to [Amazon SageMaker Built-in Algorithms](https://sagemaker.readthedocs.io/en/stable/algorithms/index.html)! You can use SageMaker Built-in algorithms to solve many Machine Learning tasks through [SageMaker Python SDK](https://sagemaker.readthedocs.io/en/stable/overview.html). You can also use these algorithms through one-click in SageMaker Studio via [JumpStart](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-jumpstart.html).\n",
    "\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK for finetuning Foundation Models and deploying the trained model for inference. The Foundation models perform Text Generation task. It takes a text string as input and predicts next words in the sequence.\n",
    "\n",
    "* **How to run inference on [GPT-J 6B](https://huggingface.co/EleutherAI/gpt-j-6b) model without finetuning.**\n",
    "* **How to fine-tune [GPT-J 6B](https://huggingface.co/EleutherAI/gpt-j-6b) model on a domain specific dataset, and then run inference on the fine-tuned model. In particular, the example dataset we demonstrated is [publicly available SEC filing](https://www.sec.gov/edgar/searchedgar/companysearch) of Amazon from year 2021 to 2022. The expectation is that after fine-tuning, the model should be able to generate insightful text in financial domain.**\n",
    "* **We compare the inference result for GPT-J 6B before finetuning and after finetuning.**\n",
    "\n",
    "Note: This notebook was tested on ml.t3.medium instance in Amazon SageMaker Studio with Python 3 (Data Science) kernel and in Amazon SageMaker Notebook instance with conda_python3 kernel.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8091e1f6",
   "metadata": {},
   "source": [
    "1. [Set Up](#1.-Set-Up)\n",
    "2. [Select Text Generation Model GTP-J 6B](#2.-Select-Text-Generation-Model-GTP-J-6B)\n",
    "3. [Run Inference on the Pre-trained Model without finetuning](#3.-Run-Inference-on-the-Pre-trained-Model-without-finetuning)\n",
    "    * [Retrieve Artifacts & Deploy an Endpoint](#3.1.-Retrieve-Artifacts-&-Deploy-an-Endpoint)\n",
    "    * [Query endpoint and parse response](#3.2.-Query-endpoint-and-parse-response)\n",
    "    * [Clean up the endpoint](#3.3.-Clean-up-the-endpoint)\n",
    "4. [Finetune the pre-trained model on a custom dataset](#4.-Fine-tune-the-pre-trained-model-on-a-custom-dataset)\n",
    "    * [Retrieve Training artifacts](#4.1.-Retrieve-Training-artifacts)\n",
    "    * [Set Training parameters](#4.2.-Set-Training-parameters)\n",
    "    * [Train with Automatic Model Tuning](#4.3.-Train-with-Automatic-Model-Tuning-([HPO]))\n",
    "    * [Start Training](#4.4.-Start-Training)\n",
    "    * [Extract Training performance metrics](#4.5.-Extract-Training-performance-metrics)\n",
    "    * [Deploy & run Inference on the fine-tuned model](#4.6.-Deploy-&-run-Inference-on-the-fine-tuned-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2007b31a",
   "metadata": {},
   "source": [
    "## 1. Set Up\n",
    "Before executing the notebook, there are some initial steps required for setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b943ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "sparkmagic 0.20.4 requires nest-asyncio==1.5.5, but you have nest-asyncio 1.5.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets==7.0.0 --quiet\n",
    "!pip install --upgrade sagemaker --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1051f6",
   "metadata": {},
   "source": [
    "To train and host on Amazon Sagemaker, we need to setup and authenticate the use of AWS services. Here, we use the execution role associated with the current notebook instance as the AWS account role with SageMaker access. It has necessary permissions, including access to your data in S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5a3eb07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee983c64",
   "metadata": {},
   "source": [
    "## 2. Select Text Generation Model GTP-J 6B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "960ca9c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"huggingface-textgeneration1-gpt-j-6b\", \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e49a7",
   "metadata": {},
   "source": [
    "## 3. Run Inference on the Pre-trained Model without finetuning\n",
    "\n",
    "Using SageMaker, we can directly perform inference on the pre-trained [GPT-J 6B](https://huggingface.co/EleutherAI/gpt-j-6b) model. GPT-J 6B  is an open source 6 billion parameter model released by Eleuther AI. GPT-J 6B has been trained on a large corpus of text data ([the Pile](https://pile.eleuther.ai/) dataset) and is capable of performing various natural language processing tasks such as text generation, text classification, and text summarization. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45f9242",
   "metadata": {},
   "source": [
    "### 3.1. Retrieve Artifacts & Deploy an Endpoint\n",
    "\n",
    "We retrieve the deploy_image_uri, deploy_source_uri, and base_model_uri for the pre-trained model. To host the pre-trained model, we create an instance of [`sagemaker.model.Model`](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html) and deploy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fecbe672",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-{model_id}\")\n",
    "\n",
    "inference_instance_type = \"ml.g5.12xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the model uri.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "# Create the SageMaker model instance. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    ")\n",
    "\n",
    "# deploy the Model. TODO\n",
    "base_model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce1248-9fa9-4617-9dfa-d432c03ad804",
   "metadata": {},
   "source": [
    "### 3.2. Query endpoint and parse response\n",
    "The model takes a text string as input and predicts next words in the sequence. We use three of following input examples.\n",
    "\n",
    "1. `This Form 10-K report shows that`\n",
    "2. `We serve consumers through`\n",
    "3. `Our vision is`\n",
    "\n",
    "**The input examples are related to company's perforamnce in financial report. You will see the outputs from the model without finetuning are limited in providing insightful contents.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f12c38b-fd58-4696-824d-be15e34f5a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response_multiple_texts(query_response):\n",
    "    generated_text = []\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    return model_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c63f536-be9b-4e52-a464-d206998e7071",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Form 10-K report shows that: (a) we, a Nevada corporation and Nevada corporation, have been registered with the Nevada Corporations Commission as an “small business” as defined by NRS 179.4160,179.3090,179.30,179.30 and 179.31 NRS;\n",
      "(b) the number of our shareholders is limited to no more than 100 shareholders;\n",
      "(c) the number of shares we issue is limited to no more than 500 shares;\n",
      "(d) the principal business of our parent company is Nevada;\n",
      "(e) the principal place of business of our company is Las Vegas, Nevada;\n",
      "(f) we conduct our principal business in the State of Nevada;\n",
      "(g) our place of incorporation is located in the State of Nevada;\n",
      "(h) the Nevada corporation code is listed as N.C.A.C.O.1;\n",
      "(i) our address is: _______________, Nevada;\n",
      "(j) our place of incorporation is: ______________, Nevada;\n",
      "7\n",
      "(k) our: principal executive office is: _________, State of Nevada;\n",
      "(l) our annual receipts, if applicable, are as follows:\n",
      "(m) we are subject to federal income tax on our income;\n",
      "(n) our name: _____________;\n",
      "(o) our: mailing address is: ________________;\n",
      "(p) our principal business in Nevada;\n",
      "(q) we are not listed in the United States Small Business Administration;\n",
      "(r) our registered address is: ______________, Nevada;\n",
      "(s) our agent for service of process is:\n",
      "\n",
      "(t) our registered address is:\n",
      "We, the undersigned, declare under penalty of perjury that the foregoing is true and correct to the best of our knowledge and belief.\n",
      "\n",
      "* * * * * * * * * * * * * *\n",
      "\n",
      "\n",
      "We serve consumers through:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "We serve businesses through:\n",
      "\n",
      "\n",
      "The World Wide Web (Web site)\n",
      "\n",
      "Website\n",
      "The Internet (e-mail)\n",
      "\n",
      "\n",
      "The Internet (e-mail)\n",
      "\n",
      "Internet e-mail\n",
      "\n",
      "\n",
      "Email\n",
      "\n",
      "Internet e-mail\n",
      "\n",
      "\n",
      "FTP (File Transfer Protocol)\n",
      "\n",
      "File Transfer Protocol (FTP)\n",
      "\n",
      "\n",
      "Internet e-mail\n",
      "\n",
      "\n",
      "\n",
      "Usenet (USENET)\n",
      "\n",
      "Usenet newsgroups\n",
      "\n",
      "Usenet newsgroups\n",
      "\n",
      "Usenet newsgroups\n",
      "\n",
      "\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "\n",
      "Usenet newsgroups\n",
      "\n",
      "Usenet usenet newsgroups\n",
      "\n",
      "\n",
      "\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "\n",
      "Usenet newsgroups\n",
      "\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "\n",
      "Usenet newsgroups\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Usenet newsgroups\n",
      "\n",
      "\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "Usenet newsgroups\n",
      "\n",
      "Usenet newsgroups\n",
      "\n",
      "\n",
      "Our vision is: “We believe that all humans are created in the image of God and as such, we hold ourselves to be a people of compassion, respect, and we will only interact with others on this basis.”\n",
      "\n",
      "The RSC will work to be active participants in the Canadian Church as it will be involved with Christian activities in the following ways:\n",
      "\n",
      "1) We will be active in our local Church, St. Michael’s Roman Catholic Parish in Edmonton, Alberta, Canada.\n",
      "\n",
      "2) We will be a part of a Christian group of believers in Christ, the Church of God.\n",
      "3) We will seek to make this our Church as close to Jesus Christ as we can be, and do everything possible to walk the path to eternal life, by the way of the Lord.\n",
      "\n",
      "4) We will be involved in the Christian community and with people who will be in prayer with us.\n",
      "\n",
      "We will endeavor to learn all the Church laws of God and do everything possible to be “a pure vessel” and walk in the way of the Lord.\n",
      "\n",
      "\n",
      "I ask you, my brothers and sisters to join me in this path.\n",
      "\n",
      "I am asking for prayer as this is going to be a very difficult time for us, a people who do not know God’s grace and blessing and salvation, with the prayers of the Church and for the souls of this group and community. I am requesting to you that you pray for us, the saints and sinners, you to be more in the name of Jesus Christ.\n",
      "\n",
      "\n",
      "\n",
      "H.M.S.H.E.T.A.T.C.S.O.T.S.S.C.C.I.C.E.C.L.O.G.S.I.A.T.\n",
      "\n",
      "\n",
      "The Holy Spirit,\n",
      "The Redeemer of All People\n",
      "\n",
      "\n",
      "The one\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"max_length\": 400,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "}\n",
    "\n",
    "res_gpt_before_finetune = []\n",
    "for quota_text in [\n",
    "    \"This Form 10-K report shows that\",\n",
    "    \"We serve consumers through\",\n",
    "    \"Our vision is\",\n",
    "]:\n",
    "    payload = {\"text_inputs\": f\"{quota_text}:\", **parameters}\n",
    "\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "    )\n",
    "    generated_texts = parse_response_multiple_texts(query_response)[0][\"generated_text\"]\n",
    "    res_gpt_before_finetune.append(generated_texts)\n",
    "    print(generated_texts)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60dbad7",
   "metadata": {},
   "source": [
    "### 3.3. Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f8225e",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint and the attached resources\n",
    "#base_model_predictor.delete_model()\n",
    "#base_model_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70950bf9",
   "metadata": {},
   "source": [
    "## 4. Fine-tune the pre-trained model on a custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1f6f8c",
   "metadata": {},
   "source": [
    "Fine-tuning refers to the process of taking a pre-trained language model and retraining it for a different but related task using specific data. This approach is also known as transfer learning, which involves transferring the knowledge learned from one task to another. Large language models (LLMs) like GPT-J 6B are trained on massive amounts of unlabeled data and can be fine-tuned on domain domain datasets, making the model perform better on that specific domain. \n",
    "\n",
    "We will use financial text from SEC filings to fine tune a LLM GPT-J 6B for financial applications. \n",
    "\n",
    "\n",
    "\n",
    "- **Input**: A train and an optional validation directory. Each directory contains a CSV/JSON/TXT file.\n",
    "    - For CSV/JSON files, the train or validation data is used from the column called 'text' or the first column if no column called 'text' is found.\n",
    "    - The number of files under train and validation (if provided) should equal to one.\n",
    "- **Output**: A trained model that can be deployed for inference.\n",
    "Below is an example of a TXT file for fine-tuning the Text Generation model. The TXT file is SEC filings of Amazon from year 2021 to 2022.\n",
    "\n",
    "---\n",
    "```\n",
    "This report includes estimates, projections, statements relating to our\n",
    "business plans, objectives, and expected operating results that are “forward-\n",
    "looking statements” within the meaning of the Private Securities Litigation\n",
    "Reform Act of 1995, Section 27A of the Securities Act of 1933, and Section 21E\n",
    "of the Securities Exchange Act of 1934. Forward-looking statements may appear\n",
    "throughout this report, including the following sections: “Business” (Part I,\n",
    "Item 1 of this Form 10-K), “Risk Factors” (Part I, Item 1A of this Form 10-K),\n",
    "and “Management’s Discussion and Analysis of Financial Condition and Results\n",
    "of Operations” (Part II, Item 7 of this Form 10-K). These forward-looking\n",
    "statements generally are identified by the words “believe,” “project,”\n",
    "“expect,” “anticipate,” “estimate,” “intend,” “strategy,” “future,”\n",
    "“opportunity,” “plan,” “may,” “should,” “will,” “would,” “will be,” “will\n",
    "continue,” “will likely result,” and similar expressions. Forward-looking\n",
    "statements are based on current expectations and assumptions that are subject\n",
    "to risks and uncertainties that may cause actual results to differ materially.\n",
    "We describe risks and uncertainties that could cause actual results and events\n",
    "to differ materially in “Risk Factors,” “Management’s Discussion and Analysis\n",
    "of Financial Condition and Results of Operations,” and “Quantitative and\n",
    "Qualitative Disclosures about Market Risk” (Part II, Item 7A of this Form\n",
    "10-K). Readers are cautioned not to place undue reliance on forward-looking\n",
    "statements, which speak only as of the date they are made. We undertake no\n",
    "obligation to update or revise publicly any forward-looking statements,\n",
    "whether because of new information, future events, or otherwise.\n",
    "\n",
    "GENERAL\n",
    "\n",
    "Embracing Our Future ...\n",
    "```\n",
    "---\n",
    "SEC filings data of Amazon is downloaded from publicly available [EDGAR](https://www.sec.gov/edgar/searchedgar/companysearch). Instruction of accessing the data is shown [here](https://www.sec.gov/os/accessing-edgar-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4552dd0",
   "metadata": {},
   "source": [
    "### 4.1. Retrieve Training artifacts\n",
    "Here, for the selected model, we retrieve the training docker container, the training algorithm source, the pre-trained model, and a python dictionary of the training hyper-parameters that the algorithm accepts with their default values. Note that the model_version=\"*\" fetches the latest model. Also, we do need to specify the training_instance_type to fetch train_image_uri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3d9fd0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "\n",
    "training_instance_type = \"ml.g5.12xlarge\"\n",
    "\n",
    "# Retrieve the docker image\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    image_scope=\"training\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "# Retrieve the training script\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"training\"\n",
    ")\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bfe06b",
   "metadata": {},
   "source": [
    "### 4.2. Set Training parameters\n",
    "Now that we are done with all the setup that is needed, we are ready to fine-tune our Text Classification model. To begin, let us create a [``sageMaker.estimator.Estimator``](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html) object. This estimator will launch the training job. \n",
    "\n",
    "There are two kinds of parameters that need to be set for training. \n",
    "\n",
    "The first one are the parameters for the training job. These include: (i) Training data path. This is S3 folder in which the input data is stored, (ii) Output path: This the s3 folder in which the training output is stored. (iii) Training instance type: This indicates the type of machine on which to run the training. Typically, we use GPU instances for these training. We defined the training instance type above to fetch the correct train_image_uri. \n",
    "***\n",
    "The second set of parameters are algorithm specific training hyper-parameters. It is also used for sepcifying the model name if we want to fine-tune on the model which is not present in the dropdown list.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "036bac37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample training data is available in this bucket\n",
    "data_bucket = f\"jumpstart-cache-prod-{aws_region}\"\n",
    "data_prefix = \"training-datasets/sec_data\"\n",
    "\n",
    "training_dataset_s3_path = f\"s3://{data_bucket}/{data_prefix}/train/\"\n",
    "validation_dataset_s3_path = f\"s3://{data_bucket}/{data_prefix}/validation/\"\n",
    "\n",
    "output_bucket = sess.default_bucket()\n",
    "output_prefix = \"jumpstart-example-tg-train\"\n",
    "\n",
    "s3_output_location = f\"s3://{output_bucket}/{output_prefix}/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad02cf3",
   "metadata": {},
   "source": [
    "***\n",
    "For algorithm specific hyper-parameters, we start by fetching python dictionary of the training hyper-parameters that the algorithm accepts with their default values. This can then be overridden to custom values.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "651f68c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': '3', 'learning_rate': '6e-06', 'per_device_train_batch_size': '4', 'per_device_eval_batch_size': '8', 'warmup_ratio': '0.1', 'instruction_tuned': 'False', 'train_from_scratch': 'False', 'fp16': 'True', 'bf16': 'False', 'evaluation_strategy': 'steps', 'eval_steps': '20', 'gradient_accumulation_steps': '2', 'logging_steps': '10', 'weight_decay': '0.2', 'load_best_model_at_end': 'True', 'max_train_samples': '-1', 'max_val_samples': '-1', 'seed': '10', 'max_input_length': '-1', 'validation_split_ratio': '0.2', 'train_data_split_seed': '0', 'preprocessing_num_workers': 'None', 'max_steps': '-1', 'gradient_checkpointing': 'True', 'early_stopping_patience': '3', 'early_stopping_threshold': '0.0', 'adam_beta1': '0.9', 'adam_beta2': '0.999', 'adam_epsilon': '1e-08', 'max_grad_norm': '1.0', 'label_smoothing_factor': '0', 'logging_first_step': 'False', 'logging_nan_inf_filter': 'True', 'save_strategy': 'steps', 'save_steps': '500', 'save_total_limit': '1', 'dataloader_drop_last': 'False', 'dataloader_num_workers': '0', 'eval_accumulation_steps': 'None', 'auto_find_batch_size': 'False', 'lr_scheduler_type': 'constant_with_warmup', 'warmup_steps': '0'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Retrieve the default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(model_id=model_id, model_version=model_version)\n",
    "\n",
    "# [Optional] Override default hyperparameters with custom values\n",
    "hyperparameters[\"epoch\"] = \"3\"\n",
    "hyperparameters[\"per_device_train_batch_size\"] = \"4\"\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5051d41",
   "metadata": {},
   "source": [
    "### 4.3. Train with Automatic Model Tuning ([HPO](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html)) <a id='AMT'></a>\n",
    "***\n",
    "Amazon SageMaker automatic model tuning, also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. It then chooses the hyperparameter values that result in a model that performs the best, as measured by a metric that you choose. We will use a [HyperparameterTuner](https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html) object to interact with Amazon SageMaker hyperparameter tuning APIs.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c271247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter\n",
    "\n",
    "# Use AMT for tuning and selecting the best model\n",
    "use_amt = False\n",
    "\n",
    "# Define objective metric, based on which the best model will be selected.\n",
    "amt_metric_definitions = {\n",
    "    \"metrics\": [{\"Name\": \"eval:loss\", \"Regex\": \"'eval_loss': ([0-9]+\\.[0-9]+)\"}],\n",
    "    \"type\": \"Minimize\",\n",
    "}\n",
    "\n",
    "# You can select from the hyperparameters supported by the model, and configure ranges of values to be searched for training the optimal model.(https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html)\n",
    "hyperparameter_ranges = {\n",
    "    \"learning_rate\": ContinuousParameter(0.00001, 0.0001, scaling_type=\"Logarithmic\")\n",
    "}\n",
    "\n",
    "# Increase the total number of training jobs run by AMT, for increased accuracy (and training time).\n",
    "max_jobs = 6\n",
    "# Change parallel training jobs run by AMT to reduce total training time, constrained by your account limits.\n",
    "# if max_jobs=max_parallel_jobs then Bayesian search turns to Random.\n",
    "max_parallel_jobs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9d2622",
   "metadata": {},
   "source": [
    "### 4.4. Start Training\n",
    "***\n",
    "We start by creating the estimator object with all the required assets and then launch the training job.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d923c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: jumpstart-example-huggingface-textgener-2023-07-14-14-32-05-455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-14 14:32:05 Starting - Starting the training job...\n",
      "2023-07-14 14:32:32 Starting - Preparing the instances for training............\n",
      "2023-07-14 14:34:12 Downloading - Downloading input data.......................................................\u001b[34m46%|████▌     | 44/96 [23:35<28:34, 32.97s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 45/96 [24:06<27:30, 32.37s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 46/96 [24:37<26:27, 31.75s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 47/96 [25:08<25:44, 31.52s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 48/96 [25:38<24:51, 31.07s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 49/96 [26:08<24:04, 30.73s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 50/96 [26:38<23:24, 30.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6544, 'learning_rate': 6e-06, 'epoch': 1.54}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 50/96 [26:38<23:24, 30.53s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 51/96 [27:07<22:38, 30.19s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 75/96 [39:36<10:34, 30.21s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 76/96 [40:05<10:01, 30.07s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 77/96 [40:34<09:25, 29.76s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 78/96 [41:04<08:53, 29.67s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 79/96 [41:34<08:28, 29.90s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 80/96 [42:04<07:58, 29.90s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.3435, 'learning_rate': 6e-06, 'epoch': 2.46}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 80/96 [42:04<07:58, 29.90s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-07-14 15:34:07,859 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-07-14 15:34:07,859 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-07-14 15:34:07,860 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-07-14 15:34:07,860 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-07-14 15:34:07,861 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-07-14 15:34:07,861 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:03<00:08,  1.76s/it]#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:07<00:09,  2.49s/it]#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:10<00:08,  2.88s/it]#033[A\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:14<00:06,  3.10s/it]#033[A\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:17<00:03,  3.22s/it]#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:21<00:00,  3.31s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.433837890625, 'eval_runtime': 24.628, 'eval_samples_per_second': 8.04, 'eval_steps_per_second': 0.284, 'epoch': 2.46}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 80/96 [42:29<07:58, 29.90s/it]\u001b[0m\n",
      "\u001b[34m#015100%|██████████| 7/7 [00:21<00:00,  3.31s/it]#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 81/96 [42:58<09:15, 37.03s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 82/96 [43:28<08:08, 34.89s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 83/96 [43:58<07:15, 33.46s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 84/96 [44:28<06:28, 32.35s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 85/96 [44:57<05:45, 31.37s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 86/96 [45:27<05:09, 30.95s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 87/96 [45:57<04:37, 30.87s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 88/96 [46:27<04:04, 30.62s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 89/96 [46:58<03:35, 30.73s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 90/96 [47:28<03:01, 30.31s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.3444, 'learning_rate': 6e-06, 'epoch': 2.77}\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 90/96 [47:28<03:01, 30.31s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 91/96 [47:58<02:30, 30.16s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 92/96 [48:28<02:00, 30.13s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 93/96 [48:58<01:30, 30.08s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 94/96 [49:28<01:00, 30.21s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 95/96 [49:58<00:30, 30.02s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 96/96 [50:29<00:00, 30.27s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2039] 2023-07-14 15:42:32,156 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2039] 2023-07-14 15:42:32,156 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m{'train_runtime': 3029.0657, 'train_samples_per_second': 1.02, 'train_steps_per_second': 0.032, 'train_loss': 0.7914133866628011, 'epoch': 2.95}\u001b[0m\n",
      "\u001b[34m100%|██████████| 96/96 [50:29<00:00, 30.27s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 96/96 [50:29<00:00, 31.55s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2868] 2023-07-14 15:42:32,165 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2868] 2023-07-14 15:42:32,165 >> Saving model checkpoint to /opt/ml/model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-07-14 15:42:32,167 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:457] 2023-07-14 15:42:32,167 >> Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-07-14 15:42:32,170 >> Configuration saved in /opt/ml/model/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:362] 2023-07-14 15:42:32,170 >> Configuration saved in /opt/ml/model/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2023-07-14 15:42:32,334 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1847] 2023-07-14 15:42:32,334 >> Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-07-14 15:42:32,335 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2171] 2023-07-14 15:42:32,335 >> tokenizer config file saved in /opt/ml/model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-07-14 15:42:32,335 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2178] 2023-07-14 15:42:32,335 >> Special tokens file saved in /opt/ml/model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[2023-07-14 15:42:43,137] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step96 is about to be saved!\u001b[0m\n",
      "\u001b[34m[2023-07-14 15:42:43,137] [INFO] [engine.py:3354:save_16bit_model] Saving model weights to /opt/ml/model/pytorch_model.bin, tag: global_step96\u001b[0m\n",
      "\u001b[34m[2023-07-14 15:42:43,137] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /opt/ml/model/pytorch_model.bin...\u001b[0m\n",
      "\u001b[34m[2023-07-14 15:42:54,426] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /opt/ml/model/pytorch_model.bin.\u001b[0m\n",
      "\u001b[34m[2023-07-14 15:42:54,426] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step96 is ready now!\u001b[0m\n",
      "\u001b[34m***** train metrics *****\u001b[0m\n",
      "\u001b[34mepoch                    =       2.95\n",
      "  train_loss               =     0.7914\n",
      "  train_runtime            = 0:50:29.06\n",
      "  train_samples            =       1030\n",
      "  train_samples_per_second =       1.02\n",
      "  train_steps_per_second   =      0.032\u001b[0m\n",
      "\u001b[34m07/14/2023 15:42:54 - INFO - __main__ -   Start Evaluation.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-07-14 15:42:54,894 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3129] 2023-07-14 15:42:54,894 >> ***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-07-14 15:42:54,894 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-07-14 15:42:54,894 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3131] 2023-07-14 15:42:54,894 >>   Num examples = 198\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3134] 2023-07-14 15:42:54,894 >>   Batch size = 8\u001b[0m\n",
      "\u001b[34m0%|          | 0/7 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 2/7 [00:03<00:08,  1.75s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 3/7 [00:06<00:09,  2.48s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 4/7 [00:10<00:08,  2.86s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 5/7 [00:14<00:06,  3.10s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 6/7 [00:17<00:03,  3.24s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:21<00:00,  3.32s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 7/7 [00:21<00:00,  3.04s/it]\u001b[0m\n",
      "\u001b[34m***** eval metrics *****\n",
      "  epoch                   =       2.95\n",
      "  eval_loss               =     0.3662\n",
      "  eval_runtime            = 0:00:24.63\n",
      "  eval_samples            =        198\n",
      "  eval_samples_per_second =      8.037\n",
      "  eval_steps_per_second   =      0.284\n",
      "  perplexity              =     1.4423\u001b[0m\n",
      "\u001b[34m[2023-07-14 15:43:23,145] [INFO] [launch.py:346:main] Process 163 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-07-14 15:43:24,146] [INFO] [launch.py:346:main] Process 165 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-07-14 15:43:24,146] [INFO] [launch.py:346:main] Process 166 exits successfully.\u001b[0m\n",
      "\u001b[34m[2023-07-14 15:43:25,148] [INFO] [launch.py:346:main] Process 164 exits successfully.\u001b[0m\n",
      "\u001b[34m2023-07-14 15:43:27,815 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-07-14 15:43:27,815 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-07-14 15:43:27,815 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-07-14 15:43:36 Uploading - Uploading generated training model"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "training_job_name = name_from_base(f\"jumpstart-example-{model_id}-transfer-learning\")\n",
    "\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \"'loss': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:loss\", \"Regex\": \"'eval_loss': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:runtime\", \"Regex\": \"'eval_runtime': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:samples_per_second\", \"Regex\": \"'eval_samples_per_second': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:eval_steps_per_second\", \"Regex\": \"'eval_steps_per_second': ([0-9]+\\.[0-9]+)\"},\n",
    "]\n",
    "\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "tg_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    max_run=360000,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=s3_output_location,\n",
    "    base_job_name=training_job_name,\n",
    "    metric_definitions=metric_definitions,\n",
    ")\n",
    "\n",
    "if use_amt:\n",
    "    hp_tuner = HyperparameterTuner(\n",
    "        tg_estimator,\n",
    "        amt_metric_definitions[\"metrics\"][0][\"Name\"],\n",
    "        hyperparameter_ranges,\n",
    "        amt_metric_definitions[\"metrics\"],\n",
    "        max_jobs=max_jobs,\n",
    "        max_parallel_jobs=max_parallel_jobs,\n",
    "        objective_type=amt_metric_definitions[\"type\"],\n",
    "        base_tuning_job_name=training_job_name,\n",
    "    )\n",
    "\n",
    "    # Launch a SageMaker Tuning job to search for the best hyperparameters\n",
    "    hp_tuner.fit({\"train\": training_dataset_s3_path, \"validation\": validation_dataset_s3_path})\n",
    "else:\n",
    "    # Launch a SageMaker Training job by passing s3 path of the training data\n",
    "    tg_estimator.fit(\n",
    "        {\"train\": training_dataset_s3_path, \"validation\": validation_dataset_s3_path}, logs=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ed581b",
   "metadata": {},
   "source": [
    "### 4.5. Extract Training performance metrics\n",
    "***\n",
    "Performance metrics such as training loss and validation accuracy/loss can be accessed through cloudwatch while the training. We can also fetch these metrics and analyze them within the notebook\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce268cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>1.696900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>1.391000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>1.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>960.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.812800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1260.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.654400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1560.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.561400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1920.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2220.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.343500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2520.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>0.344400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eval:loss</td>\n",
       "      <td>1.271484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp metric_name     value\n",
       "0        0.0  train:loss  1.696900\n",
       "1      300.0  train:loss  1.391000\n",
       "2      660.0  train:loss  1.130000\n",
       "3      960.0  train:loss  0.812800\n",
       "4     1260.0  train:loss  0.654400\n",
       "5     1560.0  train:loss  0.561400\n",
       "6     1920.0  train:loss  0.457000\n",
       "7     2220.0  train:loss  0.343500\n",
       "8     2520.0  train:loss  0.344400\n",
       "9        0.0   eval:loss  1.271484"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "if use_amt:\n",
    "    training_job_name = hp_tuner.best_training_job()\n",
    "else:\n",
    "    training_job_name = tg_estimator.latest_training_job.job_name\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2d20f9",
   "metadata": {},
   "source": [
    "## 4.6. Deploy & run Inference on the fine-tuned model\n",
    "***\n",
    "A trained model does nothing on its own. We now want to use the model to perform inference. For this example, that means predicting the class label of an input sentence. We follow the same steps as in [3. Run inference on the pre-trained model](#3.-Run-inference-on-the-pre-trained-model). We start by retrieving the artifacts for deploying an endpoint. However, instead of base_predictor, we  deploy the `tg_estimator` that we fine-tuned.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce738168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py39.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.g5.12xlarge.\n",
      "INFO:sagemaker:Creating model with name: sagemaker-jumpstart-2023-07-15-06-00-19-688\n",
      "INFO:sagemaker:Creating endpoint-config with name jumpstart-example-huggingface-textgener-2023-07-15-06-00-19-688\n",
      "INFO:sagemaker:Creating endpoint with name jumpstart-example-huggingface-textgener-2023-07-15-06-00-19-688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "inference_instance_type = \"ml.g5.12xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "endpoint_name_after_finetune = name_from_base(f\"jumpstart-example-{model_id}-\")\n",
    "\n",
    "# Use the estimator from the previous step to deploy to a SageMaker endpoint\n",
    "finetuned_predictor = (hp_tuner if use_amt else tg_estimator).deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    image_uri=deploy_image_uri,\n",
    "    endpoint_name=endpoint_name_after_finetune,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea22eef2",
   "metadata": {},
   "source": [
    "Next, we query the finetuned model using the same set of examples above, parse the response and print the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "097903dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response_multiple_texts(query_response):\n",
    "    generated_text = []\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    return model_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f541ccf7-d26e-4088-b52a-8fd0bf93bafc",
   "metadata": {},
   "source": [
    "The outputs from fine-tune model are generated as below. We can see that after being fine-tuned, the model can generate more insightful contents related to financial domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a6aa706-99d1-47c2-b712-c279a89eb63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Form 10-K report shows that:(1)    We are a public company, as defined by Rule 13a-15(c) of the SecuritiesExchange Act of 1934.  (2)    The following is a summary of certain material contract provisions thataffect our ability to meet our contractual commitments. This summary is notcomplete and is qualified in its entirety by reference to the entirecontract.  (1)    The agreements may be amended, modified, terminated, or otherwisechanged only by written instrument signed by the party or parties affectedand, in the case of the JV Agreements, by the parties thereto.  (2)    In the event we fail to meet our contractual obligations for anyreason, including as a result of acts of God, extreme weather, fire, flood,earthquakes, acts of war or terrorism, labor or material shortages, acts ofgovernment or regulatory authority, or other events, the other party has theright to terminate this Agreement by notifying us of such termination. Thereafter,the other party may elect to pursue any available remedy at law or equity toenforce the obligations under this Agreement.  (3)    If the JV Agreements terminate, the Company will pay the other partyreasonable compensation for the other party's interest in the terminatedJV Agreement.  (4)    If we terminate this Agreement, we will pay to the other party theamount that the other party's account balances as of the termination dateequals or exceeds the other party's costs, excluding development costs,related to the portion of the JV Activity for which the other party hastaken over responsibility upon termination.  (5)    If we terminate this Agreement prior to expiration of the term hereof,we will pay to the other party the amount determined by multiplying theexcess of (a) the amount by which the other\n",
      "\n",
      "\n",
      "We serve consumers through: Amazon Marketplace, a store offering more than 60 million products by over 150,000 sellers; Amazon Web Services, which provides technology services that enable virtually any type of business, from start-ups to global corporations; Amazon Enterprise Solutions, which offer a broad set of integrated technologies and services to better manage the enterprise customer experience, including enterprise sales, customer service, and technical support; and digital content offerings, including audiobooks, digital music, digital videos, e-books, and other non-AWS services. We serve businesses of all sizes through: AWS, which offers a broad set of global compute, storage, database, and other service offerings, and a global network, and enables virtually any type of business; merchant.com, which enables businesses to sell and customers to buy virtually anything, including through their own websites; seller.com, which enables individuals and businesses to sell virtually anything to each other through a worldwide network of websites; digital media offerings, which include movies, TV episodes, music, digital games, and other content; and co-branded credit card agreements. We serve developers and enterprises of all sizes through our developer programs, which include AWS SDKs, Xcode, Android Studio, and the Amazon Web Services Mobile Platform; and enterprise services, which include marketing and promotional, fulfillment, and payment processing services.In addition, we generate revenue through a variety of services, such as advertising, and co-branded credit card agreements.We have organized our operations into three segments: North America, International, and Amazon Web Services (“AWS”). See “Note 10—Segment Information.”Prior Period ReclassificationsCertain prior period amounts have been reclassified to conform to the current period presentation.Effect of Exchange RateChanges in the exchange rate between the U.S. Dollar and the Euro have had aneffect on our consolidated statements of operations. See “Effect of Exchange RateChanges on Financial Results.”F\n",
      "\n",
      "\n",
      "Our vision is:To be Earth’s most customer-centric company. We are transforming ourbusinesses and operating models to meet the diverse needs of our customers. Weare fully committed to being Earth’s most customer-centric company. We will dothis by ensuring our customers can find and access the products and servicesthey need, when they need them, in ways that are meaningful and convenient.We will achieve this by relentlessly focusing on our customers and meetingtheir unmet needs, both now and over time. We seek to accomplish this througha continuous cycle of research and development to innovate our products andservices, improve our operations, and to create sustainable growth in ourrevenue and profit. We are committed to remaining relentlessly focused onour customers and meeting their unmet needs. Our customers, in turn, help usdecide what we should develop, what technologies we should employ, and whichproducts and services we should offer.As our businesses continue to grow, we will focus on in-line growth ratherthan seeking to acquire or form alliances or partnerships in other industriesor countries.We seek to reduce our net debt to long-term cash flow in 2020, and if weexceed this goal, we seek to fund additional working capital needs throughoperating cash flows.Net Interest ExpenseNet interest expense was $4.2 billion, $3.8 billion, and $3.7 billion for2020, 2019, and 2018. The increase in net interest expense in 2020 isprimarily due to increases in our long-term debt and capital and financingspending. Our long-term debt was $67.8 billion as of December 31, 2020,compared to $48.4 billion as of December 31, 2019 and $39.7 billion asof December 31, 2018.Our long-term debt is comprised of the following (in millions):8  Free cash flow, a non-GAAP financial measure, is defined as net cashprovided by operating activities\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"max_length\": 400,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "}\n",
    "\n",
    "res_gpt_finetune = []\n",
    "for quota_text in [\n",
    "    \"This Form 10-K report shows that\",\n",
    "    \"We serve consumers through\",\n",
    "    \"Our vision is\",\n",
    "]:\n",
    "    payload = {\"text_inputs\": f\"{quota_text}:\", **parameters}\n",
    "\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name_after_finetune\n",
    "    )\n",
    "    generated_texts = parse_response_multiple_texts(query_response)[0][\"generated_text\"]\n",
    "    res_gpt_finetune.append(generated_texts)\n",
    "    print(generated_texts)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc10989f-71f7-4f0e-ba5c-5dd7b375cf69",
   "metadata": {},
   "source": [
    "We compare the outputs between the model before fine-tuning and after fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "280a1d7c-4236-4dd5-aeab-912ff582d498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Input example</th>\n",
       "      <th>Output before finetuning</th>\n",
       "      <th>Output after finetuning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This Form 10-K report shows that</td>\n",
       "      <td>This Form 10-K report shows that: (a) we, a Ne...</td>\n",
       "      <td>This Form 10-K report shows that:(1)    We are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We serve consumers through</td>\n",
       "      <td>We serve consumers through:\\n\\n\\n\\n\\nWe serve ...</td>\n",
       "      <td>We serve consumers through: Amazon Marketplace...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Our vision is</td>\n",
       "      <td>Our vision is: “We believe that all humans are...</td>\n",
       "      <td>Our vision is:To be Earth’s most customer-cent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Input example  \\\n",
       "0  This Form 10-K report shows that   \n",
       "1        We serve consumers through   \n",
       "2                     Our vision is   \n",
       "\n",
       "                            Output before finetuning  \\\n",
       "0  This Form 10-K report shows that: (a) we, a Ne...   \n",
       "1  We serve consumers through:\\n\\n\\n\\n\\nWe serve ...   \n",
       "2  Our vision is: “We believe that all humans are...   \n",
       "\n",
       "                             Output after finetuning  \n",
       "0  This Form 10-K report shows that:(1)    We are...  \n",
       "1  We serve consumers through: Amazon Marketplace...  \n",
       "2  Our vision is:To be Earth’s most customer-cent...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Input example\": [\n",
    "            \"This Form 10-K report shows that\",\n",
    "            \"We serve consumers through\",\n",
    "            \"Our vision is\",\n",
    "        ],\n",
    "        \"Output before finetuning\": res_gpt_before_finetune,\n",
    "        \"Output after finetuning\": res_gpt_finetune,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e4a026e-a0cd-40bb-8d13-63b38f28a2da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This Form 10-K report shows that: (a) we, a Nevada corporation and Nevada corporation, have been registered with the Nevada Corporations Commission as an “small business” as defined by NRS 179.4160,179.3090,179.30,179.30 and 179.31 NRS;\n",
      "(b) the number of our shareholders is limited to no more than 100 shareholders;\n",
      "(c) the number of shares we issue is limited to no more than 500 shares;\n",
      "(d) the principal business of our parent company is Nevada;\n",
      "(e) the principal place of business of our company is Las Vegas, Nevada;\n",
      "(f) we conduct our principal business in the State of Nevada;\n",
      "(g) our place of incorporation is located in the State of Nevada;\n",
      "(h) the Nevada corporation code is listed as N.C.A.C.O.1;\n",
      "(i) our address is: _______________, Nevada;\n",
      "(j) our place of incorporation is: ______________, Nevada;\n",
      "7\n",
      "(k) our: principal executive office is: _________, State of Nevada;\n",
      "(l) our annual receipts, if applicable, are as follows:\n",
      "(m) we are subject to federal income tax on our income;\n",
      "(n) our name: _____________;\n",
      "(o) our: mailing address is: ________________;\n",
      "(p) our principal business in Nevada;\n",
      "(q) we are not listed in the United States Small Business Administration;\n",
      "(r) our registered address is: ______________, Nevada;\n",
      "(s) our agent for service of process is:\n",
      "\n",
      "(t) our registered address is:\n",
      "We, the undersigned, declare under penalty of perjury that the foregoing is true and correct to the best of our knowledge and belief.\n",
      "\n",
      "* * * * * * * * * * * * * *\n",
      "We serve consumers through: Amazon Marketplace, a store offering more than 60 million products by over 150,000 sellers; Amazon Web Services, which provides technology services that enable virtually any type of business, from start-ups to global corporations; Amazon Enterprise Solutions, which offer a broad set of integrated technologies and services to better manage the enterprise customer experience, including enterprise sales, customer service, and technical support; and digital content offerings, including audiobooks, digital music, digital videos, e-books, and other non-AWS services. We serve businesses of all sizes through: AWS, which offers a broad set of global compute, storage, database, and other service offerings, and a global network, and enables virtually any type of business; merchant.com, which enables businesses to sell and customers to buy virtually anything, including through their own websites; seller.com, which enables individuals and businesses to sell virtually anything to each other through a worldwide network of websites; digital media offerings, which include movies, TV episodes, music, digital games, and other content; and co-branded credit card agreements. We serve developers and enterprises of all sizes through our developer programs, which include AWS SDKs, Xcode, Android Studio, and the Amazon Web Services Mobile Platform; and enterprise services, which include marketing and promotional, fulfillment, and payment processing services.In addition, we generate revenue through a variety of services, such as advertising, and co-branded credit card agreements.We have organized our operations into three segments: North America, International, and Amazon Web Services (“AWS”). See “Note 10—Segment Information.”Prior Period ReclassificationsCertain prior period amounts have been reclassified to conform to the current period presentation.Effect of Exchange RateChanges in the exchange rate between the U.S. Dollar and the Euro have had aneffect on our consolidated statements of operations. See “Effect of Exchange RateChanges on Financial Results.”F\n"
     ]
    }
   ],
   "source": [
    "print(res_gpt_before_finetune[0])\n",
    "print(res_gpt_finetune[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3d9168",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we clean up the deployed endpoint.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f98c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint and the attached resources\n",
    "#finetuned_predictor.delete_model()\n",
    "#finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23f8a02",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-east-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/us-west-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ca-central-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/sa-east-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-west-3/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-central-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/eu-north-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-southeast-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-northeast-2/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://h75twx4l60.execute-api.us-west-2.amazonaws.com/sagemaker-nb/ap-south-1/introduction_to_amazon_algorithms|jumpstart-foundation-models|domain-adaption-finetuning-gpt-j-6b.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
